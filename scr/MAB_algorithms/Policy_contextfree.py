import random
import numpy as np
import pandas as pd
import time
import os
import json
import pymc

opt = 'https://www.petspyjamas.com/travel/nav/dog-friendly-cottages-hot-tub'

class EpsilonGreedy():
    """
    EpsilonGreedy
    epsilon: control the probabilty of exploration
    arms: item pool
    """
    def __init__(self, epsilon, arms):
        self.arms = arms #list of blocks
        self.k = len(arms) 
        self.n = 0 # Step count
        self.k_n = np.zeros(self.k) # set count for each arm
        self.k_reward = np.zeros(self.k)# average amount of reward we've gotten from each arms
        self.mean_reward = 0 # Total mean reward
        self.epsilon = epsilon 
        self.a = 0 #store the temporal chosen arm
        return

    def select_arm(self):
        # random.random(): generate a random number
        if random.random() > self.epsilon:
            # randomly break ties, np.argmax return the first occurence of maximum.
            # get all occurences of the max and randomly select between them
            max_p_t = np.max(self.k_reward) 
            max_idxs = np.argwhere(self.k_reward == max_p_t).flatten() 
            self.a = np.random.choice(max_idxs)
        else:
            #randomly select one arm
            self.a = np.random.choice(self.k)
        return

    def update(self, reward):
        """
        update the average reward for each arm, as well as all the arm
        """
        self.k_n[self.a] += 1
        self.n += 1
        self.mean_reward = self.mean_reward + (reward - self.mean_reward) / self.n
        self.k_reward[self.a] = self.k_reward[self.a] + (reward - self.k_reward[self.a])\
                                /self.k_n[self.a]
        return

    def reset(self):
        # Reset results while keeping settings
        self.n = 0
        self.k_n = np.zeros(self.k)
        self.mean_reward = 0
        self.k_reward = np.zeros(self.k)
        return

    
    def policy_evaluator(self,views):
        # read existing logfile
        blocks = self.arms
        mean_reward = []
        views_copy = views.copy()
        range_x = len(views)
        for i in range(range_x):
            href = views_copy.pop(random.choice(range(len(views_copy)))) #randomly pop one record
            #href = views_copy.pop(0) # pop one record in order
            self.select_arm() # select one arm by current strategy
            if self.arms[self.a] == href:
                #if recommended item matches the one generated by log policy
                reward = 1
            else:
                reward = 0
            
            self.update(reward) #update strategy
            mean_reward.append(self.mean_reward)
            #if i%5000==0:
        self.reset()
        return mean_reward

class UCB1():
    """
    UCB1
    c: set as sqrt(2) for UCB1
    arms: item pool
    """
    def __init__(self, arms, c):
        self.arms = arms
        self.k = len(arms)
        self.c = c
        self.n = 0 
        self.k_n = np.zeros(self.k)  # set count for each arm
        self.k_reward = np.zeros(self.k)  # average amount of reward we've gotten from each arms
        self.mean_reward = 0 # Total mean reward
        self.a = 0 #store the temporal chosen arm
        return

    def select_arm(self):
        #explore all arms
        for arm in range(self.k):
            if self.k_n[arm] == 0:
                self.a = arm
                return 

        ucb_values = [0.0 for arm in range(self.k)]
        #compute the expected reward for each arm
        for arm in range(self.k):
            ucb_values[arm] = self.k_reward[arm] + self.c * (np.sqrt(np.log(self.n)) / float(self.k_n[arm]))
        # randomly break ties, np.argmax return the first occurence of maximum.
        # get all occurences of the max and randomly select between them
        max_p_t = np.max(ucb_values)
        max_idxs = np.argwhere(ucb_values == max_p_t).flatten()
        self.a = np.random.choice(max_idxs) 
        return 

    def update(self, reward):
        """
        update the average reward for each arm, as well as all the arm
        """
        self.k_n[self.a] += 1
        self.n += 1
        self.mean_reward = self.mean_reward + (reward - self.mean_reward) / self.n
        self.k_reward[self.a] = self.k_reward[self.a] + (reward - self.k_reward[self.a])\
                                /self.k_n[self.a]
        return

    def reset(self):
        # Resets results while keeping settings
        self.n = 0
        self.mean_reward = 0
        self.k_n = np.zeros(self.k)
        self.k_reward = np.zeros(self.k)
        return

    def policy_evaluator(self,views):
        # read existing logfile
        blocks = self.arms
        mean_reward = []
        views_copy = views.copy()
        range_x = len(views)
        for i in range(range_x):
            href = views_copy.pop(random.choice(range(len(views_copy)))) #randomly pop one record
            #href = views_copy.pop(0) # pop one record in order
            self.select_arm() # select one arm by current strategy
            if self.arms[self.a] == href:
                #if recommended item matches the one generated by log policy
                reward = 1
            else:
                reward = 0
            self.update(reward) #update strategy
            mean_reward.append(self.mean_reward)
        self.reset()
        return mean_reward

    
class Thompson_sampling():
    """
    Thompson_sampling with beta distribution

    """
    def __init__(self, arms):
        self.arms = arms
        self.k = len(arms)
        self.n = 0
        self.k_reward = np.zeros(self.k)  # set count for each arm
        self.k_n = np.zeros(self.k)  # average amount of reward we've gotten from each arms
        self.wins = np.zeros(self.k) # number of times each arm getting recommended 
        self.mean_reward = 0 # Total mean reward
        self.a = 0 #store the temporal chosen arm
        return

    def select_arm(self):
        ts_values = pymc.rbeta(1 + self.wins, 1 + self.k_n - self.wins)
        # randomly break ties, np.argmax return the first occurence of maximum.
        # get all occurences of the max and randomly select between them
        max_p_t = np.max(ts_values)
        max_idxs = np.argwhere(ts_values == max_p_t).flatten()
        self.a = np.random.choice(max_idxs) 
        return

    def update(self, reward):
        """
        update the average reward for each arm, as well as all the arm
        update the number of times getting recommended of the pulled arm
        """
        self.n += 1
        self.k_n[self.a] += 1
        self.mean_reward = self.mean_reward + (reward - self.mean_reward) / self.n
        self.k_reward[self.a] = self.k_reward[self.a] + (reward - self.k_reward[self.a])\
                                /self.k_n[self.a]
        self.wins[self.a] = self.k_reward[self.a] * self.k_n[self.a]
        return

    def reset(self):
        # Resets results while keeping settings
        self.n = 0
        self.mean_reward = 0
        self.k_n = np.zeros(self.k)
        self.k_reward = np.zeros(self.k)
        self.wins = np.zeros(self.k) 
        return

    def policy_evaluator(self,views):
        # read existing logfile
        blocks = self.arms
        mean_reward = []
        views_copy = views.copy()
        range_x = len(views)
        for i in range(range_x):
            href = views_copy.pop(random.choice(range(len(views_copy)))) #randomly pop one record
            #href = views_copy.pop(0) # pop one record in order
            self.select_arm() # select one arm by current strategy
            if self.arms[self.a] == href:
                #if recommended item matches the one generated by log policy
                reward = 1
            else:
                reward = 0
            
            self.update(reward) #update strategy
            mean_reward.append(self.mean_reward)
        self.reset()
        return mean_reward

    